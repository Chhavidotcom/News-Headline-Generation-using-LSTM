{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#importing libraraies\n\n# keras module for building LSTM \nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nimport keras.utils as ku \n\n# set seeds for reproducability\nfrom tensorflow import set_random_seed\nfrom numpy.random import seed\nset_random_seed(2)\nseed(1)\n\nimport pandas as pd\nimport numpy as np\nimport string, os \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-24T10:30:29.767928Z","iopub.execute_input":"2021-09-24T10:30:29.768422Z","iopub.status.idle":"2021-09-24T10:30:29.776275Z","shell.execute_reply.started":"2021-09-24T10:30:29.768222Z","shell.execute_reply":"2021-09-24T10:30:29.775293Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"\n#Load the dataset of news headlines\ncurr_dir = '../input/'\nall_headlines = []\nfor filename in os.listdir(curr_dir):\n    if 'Articles' in filename:\n        article_df = pd.read_csv(curr_dir + filename)\n        all_headlines.extend(list(article_df.headline.values))\n        break\n\nall_headlines = [h for h in all_headlines if h != \"Unknown\"]\nlen(all_headlines)","metadata":{"_uuid":"87836e3adbe046dd0db62013491ba62bae93b6be","_cell_guid":"b8ef1429-ff19-4a6c-92d7-af8cc61c55f7","execution":{"iopub.status.busy":"2021-09-24T10:30:29.777627Z","iopub.execute_input":"2021-09-24T10:30:29.778166Z","iopub.status.idle":"2021-09-24T10:30:29.806017Z","shell.execute_reply.started":"2021-09-24T10:30:29.778119Z","shell.execute_reply":"2021-09-24T10:30:29.805147Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"\n\n### Dataset cleaning \n\nIn dataset preparation step, we will first perform text cleaning of the data which includes removal of punctuations and lower casing all the words. ","metadata":{"_uuid":"fda5d4868631d3618d4d9a9a863541b2faf121c0","_cell_guid":"9dbd8bc9-fb61-43b9-b0c4-98bd7f3f8150"}},{"cell_type":"code","source":"def clean_text(txt):\n    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n    return txt \n\ncorpus = [clean_text(x) for x in all_headlines]\ncorpus[:10]","metadata":{"_uuid":"2a07365a27a7ba2f92fc9ba4d05d8e6254a68d8c","_cell_guid":"b8bf84ed-da11-4f89-a584-9dceea677420","execution":{"iopub.status.busy":"2021-09-24T10:30:29.807244Z","iopub.execute_input":"2021-09-24T10:30:29.807543Z","iopub.status.idle":"2021-09-24T10:30:29.828582Z","shell.execute_reply.started":"2021-09-24T10:30:29.807496Z","shell.execute_reply":"2021-09-24T10:30:29.827455Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"###  Generating Sequence of N-gram Tokens\n\n \n","metadata":{"_uuid":"6fd11859fd71aa5c7ce10bdbbd31c8eb6d1b3118","_cell_guid":"9d83cc08-19ba-4b00-9ca6-dcf5ff39c8af"}},{"cell_type":"code","source":"#tokenization\ntokenizer = Tokenizer()\n\ndef get_sequence_of_tokens(corpus):\n    ## tokenization\n    tokenizer.fit_on_texts(corpus)\n    total_words = len(tokenizer.word_index) + 1\n    \n    ## convert data to sequence of tokens \n    input_sequences = []\n    for line in corpus:\n        token_list = tokenizer.texts_to_sequences([line])[0]\n        for i in range(1, len(token_list)):\n            n_gram_sequence = token_list[:i+1]\n            input_sequences.append(n_gram_sequence)\n    return input_sequences, total_words\n\ninp_sequences, total_words = get_sequence_of_tokens(corpus)\ninp_sequences[:10]","metadata":{"_uuid":"9129a8b773feb72eff91aa0025157a173d10c625","_cell_guid":"896543c9-7944-4748-b8ef-ef8cbc2a84f0","execution":{"iopub.status.busy":"2021-09-24T10:30:29.830401Z","iopub.execute_input":"2021-09-24T10:30:29.830877Z","iopub.status.idle":"2021-09-24T10:30:29.888643Z","shell.execute_reply.started":"2021-09-24T10:30:29.830650Z","shell.execute_reply":"2021-09-24T10:30:29.887862Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"def generate_padded_sequences(input_sequences):\n    max_sequence_len = max([len(x) for x in input_sequences])\n    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n    \n    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n    label = ku.to_categorical(label, num_classes=total_words)\n    return predictors, label, max_sequence_len\n\npredictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)","metadata":{"_uuid":"ca588b414e70e21bebcead960f6632805d37dd8c","_cell_guid":"73254551-40bd-45b1-a7a5-88fe4cbe0b20","execution":{"iopub.status.busy":"2021-09-24T10:30:29.890130Z","iopub.execute_input":"2021-09-24T10:30:29.890699Z","iopub.status.idle":"2021-09-24T10:30:29.963076Z","shell.execute_reply.started":"2021-09-24T10:30:29.890561Z","shell.execute_reply":"2021-09-24T10:30:29.962030Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"#using LSTM MODEL\ndef create_model(max_sequence_len, total_words):\n    input_len = max_sequence_len - 1\n    model = Sequential()\n    \n    # Add Input Embedding Layer\n    model.add(Embedding(total_words, 10, input_length=input_len))\n    \n    # Add Hidden Layer 1 - LSTM Layer\n    model.add(LSTM(100))\n    model.add(Dropout(0.1))\n    \n    # Add Output Layer\n    model.add(Dense(total_words, activation='softmax'))\n\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    \n    return model\n\nmodel = create_model(max_sequence_len, total_words)\nmodel.summary()","metadata":{"_uuid":"76ef6d9352002d333a7c75e8aed7ce996015f527","_cell_guid":"60d6721e-e40e-4f2b-8f63-c06459d68f26","execution":{"iopub.status.busy":"2021-09-24T10:30:29.964658Z","iopub.execute_input":"2021-09-24T10:30:29.965324Z","iopub.status.idle":"2021-09-24T10:30:30.555797Z","shell.execute_reply.started":"2021-09-24T10:30:29.965056Z","shell.execute_reply":"2021-09-24T10:30:30.554905Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"Lets train our model now","metadata":{"_uuid":"f0b16b471969dbb831cb0024e303341e11b63de4","_cell_guid":"1826aa1a-cb77-4379-a69d-e9b180945dce"}},{"cell_type":"code","source":"model.fit(predictors, label, epochs=100, verbose=5)","metadata":{"_uuid":"156f3303b8120cc6932e6db985cbea4a7ceb08bf","_cell_guid":"07d5cf03-d171-4993-9f8b-18446649ecb0","execution":{"iopub.status.busy":"2021-09-24T10:30:30.557011Z","iopub.execute_input":"2021-09-24T10:30:30.557274Z","iopub.status.idle":"2021-09-24T10:36:09.454627Z","shell.execute_reply.started":"2021-09-24T10:30:30.557228Z","shell.execute_reply":"2021-09-24T10:36:09.453805Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"## Generating the text \n\n","metadata":{"_uuid":"448bf43b123060dfe4e27cb9f12889e4fe0ed2a7","_cell_guid":"61e99cfe-7395-4d61-8d1a-8539103d3db5"}},{"cell_type":"code","source":"def generate_text(seed_text, next_words, model, max_sequence_len):\n    for _ in range(next_words):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n        predicted = model.predict_classes(token_list, verbose=0)\n        \n        output_word = \"\"\n        for word,index in tokenizer.word_index.items():\n            if index == predicted:\n                output_word = word\n                break\n        seed_text += \" \"+output_word\n    return seed_text.title()","metadata":{"_uuid":"e71e56543b7065f115a05e3fd062262b3b94ad46","execution":{"iopub.status.busy":"2021-09-24T10:36:09.457666Z","iopub.execute_input":"2021-09-24T10:36:09.457961Z","iopub.status.idle":"2021-09-24T10:36:09.463501Z","shell.execute_reply.started":"2021-09-24T10:36:09.457911Z","shell.execute_reply":"2021-09-24T10:36:09.462781Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"## Some Results","metadata":{"_uuid":"c49bf4ea0e54f3145149e164e243d897f545b84c","_cell_guid":"ea0bddb6-acc6-4592-a2e0-ffc4129a582f"}},{"cell_type":"code","source":"print (generate_text(\"India\", 5, model, max_sequence_len))\nprint (generate_text(\"Avengers\", 8, model, max_sequence_len))\nprint (generate_text(\"donald trump\", 4, model, max_sequence_len))\nprint (generate_text(\"india and china\", 7, model, max_sequence_len))\nprint (generate_text(\"coding\", 4, model, max_sequence_len))\nprint (generate_text(\"science and technology\", 5, model, max_sequence_len))","metadata":{"_uuid":"a21548224c9e661a29e3d369e348aada0599bdc9","_cell_guid":"e38dd280-093b-4091-b82b-9aa90045b107","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-24T10:36:09.465203Z","iopub.execute_input":"2021-09-24T10:36:09.465819Z","iopub.status.idle":"2021-09-24T10:36:10.135815Z","shell.execute_reply.started":"2021-09-24T10:36:09.465769Z","shell.execute_reply":"2021-09-24T10:36:10.135217Z"},"trusted":true},"execution_count":57,"outputs":[]}]}